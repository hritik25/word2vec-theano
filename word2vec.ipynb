{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 645M (CNMeM is disabled, cuDNN 5105)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Here, as I am following the paper titled \"word2vec Parameter Learning Explained\" by Xin Ron,\n",
    "it will be referred to as [1] from here onward in subsequent comments for the code.\n",
    "\"\"\"\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimension of the embedding vector\n",
    "embeddingSize = 64\n",
    "vocabSize = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = dataset.produceWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['harry',\n",
       " 'potter',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sorcerers',\n",
       " 'stone',\n",
       " 'by',\n",
       " 'j',\n",
       " 'k',\n",
       " 'rowling']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point in the code, we have a list 'words' of all words present in our text.\n",
    "## Now,  we need to create a dataset from 'words' to use in training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23091"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "len(collections.Counter(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dataset...\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "# CREATE DATASET\n",
    "print \"creating dataset...\" \n",
    "def createDataset(words):\n",
    "    \"\"\"\n",
    "    RETURN :  \n",
    "    \n",
    "    data- \n",
    "        A list of tags for each word, denoting its rank by count in the corpus of text availbale.\n",
    "        By rank I mean a word's place when they have been sorted according to their count of occurences in the text.\n",
    "        So for example, if the word 'I' has rank 20 and the word 'am' has rank 60, then a portion of the text 'I am'\n",
    "        will be returned as [ 20, 60 ]\n",
    "    \"\"\"\n",
    "    # I will use 0 for rank of words not in the top <vocabSize> words by their counts, and call them 'rare words'\n",
    "    \n",
    "    # here I use the symbol 'UNK' for the rare words\n",
    "    counts = [['UNK', -1]]\n",
    "    counts.extend(collections.Counter(words).most_common(vocabSize - 1))\n",
    "    ranks = dict()\n",
    "    for word, count in counts:\n",
    "        ranks[word] = len(ranks)\n",
    "    data = list()\n",
    "    # for keeping count of rare words\n",
    "    rareCount = 0\n",
    "    for word in words:\n",
    "        if word in ranks:\n",
    "            index = ranks[word]\n",
    "        else:\n",
    "            index = 0  # ranks['UNK']\n",
    "            rareCount += 1\n",
    "        data.append(index)\n",
    "    counts[0][1] = rareCount\n",
    "    ranksToWords = dict(zip(ranks.values(), ranks.keys()))\n",
    "    return data, counts, ranks, ranksToWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 21052], ('the', 51920), ('and', 27607), ('to', 26856), ('of', 21844)]\n"
     ]
    }
   ],
   "source": [
    "data, counts, ranks, ranksToWords = createDataset(words)\n",
    "del words  # Hint to reduce memory.\n",
    "print 'Most common words (+UNK)', counts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('selfish', 4),\n",
       " ('comparing', 4),\n",
       " ('24', 4),\n",
       " ('25', 4),\n",
       " ('26', 4),\n",
       " ('23', 4),\n",
       " ('28', 4),\n",
       " ('29', 4),\n",
       " ('mystry', 4),\n",
       " ('harp', 4),\n",
       " ('conducted', 4),\n",
       " ('doctors', 4),\n",
       " ('theft', 4),\n",
       " ('sill', 4),\n",
       " ('forthcoming', 4),\n",
       " ('fans', 4),\n",
       " ('distraught', 4),\n",
       " ('kidnapped', 4),\n",
       " ('craved', 4),\n",
       " ('vindictive', 4),\n",
       " ('discipline', 4),\n",
       " ('dismally', 4),\n",
       " ('midsentence', 4),\n",
       " ('harassed', 4),\n",
       " ('coil', 4),\n",
       " ('accuse', 4),\n",
       " ('numbness', 4),\n",
       " ('enviously', 4),\n",
       " ('f', 4),\n",
       " ('lamb', 4),\n",
       " ('orblike', 4),\n",
       " ('bins', 4),\n",
       " ('acceleration', 4),\n",
       " ('mix', 4),\n",
       " ('whooshed', 4),\n",
       " ('relevant', 4),\n",
       " ('ushering', 4),\n",
       " ('bulk', 4),\n",
       " ('bull', 4),\n",
       " ('volunteer', 4),\n",
       " ('fleshy', 4),\n",
       " ('obliterated', 4),\n",
       " ('vegetables', 4),\n",
       " ('filius', 4),\n",
       " ('phial', 4),\n",
       " ('jack', 4),\n",
       " ('roomy', 4),\n",
       " ('represented', 4),\n",
       " ('swings', 4),\n",
       " ('noting', 4)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sample data', ['harry', 'potter', 'and', 'the', 'sorcerers', 'stone', 'by', 'j', 'k', 'rowling'])\n",
      "('Sample data', [7, 134, 2, 1, 2788, 347, 72, 5682, 6750, 6873])\n"
     ]
    }
   ],
   "source": [
    "print('Sample data', [ranksToWords[i] for i in data[:10]])\n",
    "print('Sample data', [i for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# class for a table which will be used to draw out negative samples.\n",
    "class tableForNegativeSamples:\n",
    "    def __init__(self, counts):\n",
    "        # from Mikolov et al.'s original word2vec implementation, where they use \n",
    "        # a unigram distribution raised to power 3/4 to construct negative samples\n",
    "        power = 0.75\n",
    "        norm = sum([math.pow(t[1], power) for t in counts]) # Normalizing constants\n",
    "        \n",
    "        # tableSize should be big enough so that the minimum probability, i.e. \n",
    "        # (unigram)^(3/4) for a word multiplied by tableSize comes out to be atleast 1.\n",
    "        tableSize = 1e8\n",
    "        table = np.zeros(tableSize, dtype=np.uint16)\n",
    "\n",
    "        p = 0 # Cumulative probability\n",
    "        i = 0\n",
    "        for word, count in counts:\n",
    "            p += float(math.pow(count, power))/norm\n",
    "            # fill the word in the table in the between the \n",
    "            # markings drawn out by cumulative probabilities\n",
    "            while i < tableSize and float(i) / tableSize < p:\n",
    "                table[i] = ranks[word]\n",
    "                i += 1\n",
    "        self.table = table\n",
    "\n",
    "    def sample(self, k):\n",
    "        indices = np.random.randint(low=0, high=len(self.table), size=k)\n",
    "        return [self.table[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:13: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "table = tableForNegativeSamples(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1067, 485, 5687, 954, 8710]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# come back and modify this function to include negative samples in a batch\n",
    "import random\n",
    "\n",
    "def generateBatch(positiveSampleSize, skipWindow, kNegativeSamples):\n",
    "    \"\"\"\n",
    "    PARAMETERS : \n",
    "    positiveSampleSize - length of the window which will be sliding on the\n",
    "                    continuous stream of words to generate a batch\n",
    "    skipWindow - the number of context words to be considered on either \n",
    "                either side of the taget word\n",
    "    \n",
    "    RETURNS :\n",
    "    batch - list of length = positiveSampleSize*(1 + kNegativeSamples)\n",
    "            consisting of tuples (target, context) and including negative\n",
    "            samples\n",
    "    labels - list of 0s, 1s. 1 for positive sample, 0 for negative sample.\n",
    "    \"\"\"    \n",
    "    global dataIndex\n",
    "    assert positiveSampleSize % (2*skipWindow) == 0\n",
    "    batch = []\n",
    "    labels = []\n",
    "    span = 2 * skipWindow + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[dataIndex])\n",
    "        dataIndex = (dataIndex + 1) % len(data)\n",
    "    for i in range(positiveSampleSize/(2*skipWindow)):\n",
    "        context = skipWindow  # target label at the center of the buffer\n",
    "        contextsToAvoid = [skipWindow]\n",
    "        for j in range(2*skipWindow):\n",
    "            while context in contextsToAvoid:\n",
    "                context = random.randint(0, span-1)\n",
    "            contextsToAvoid.append(context)\n",
    "            positiveSample = (buffer[skipWindow], buffer[context])\n",
    "            batch.append(positiveSample)\n",
    "            labels.append(1)\n",
    "            # attach negative samples\n",
    "            negativeSamples = table.sample(kNegativeSamples)\n",
    "            for i in range(kNegativeSamples):\n",
    "                negativeSample = (buffer[skipWindow], negativeSamples[i])\n",
    "                batch.append(negativeSample)\n",
    "                labels.append(0)\n",
    "        buffer.append(data[dataIndex])\n",
    "        dataIndex = (dataIndex + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 'potter', '->', 2, 'and')\n",
      "(134, 'potter', '->', 3, 'to')\n",
      "(134, 'potter', '->', 204, 'few')\n",
      "(134, 'potter', '->', 306, 'am')\n",
      "(134, 'potter', '->', 207, 'mcgonagall')\n",
      "(134, 'potter', '->', 6591, 'moodily')\n",
      "(134, 'potter', '->', 7, 'harry')\n",
      "(134, 'potter', '->', 1040, 'marble')\n",
      "(134, 'potter', '->', 436, 'hadnt')\n",
      "(134, 'potter', '->', 1651, 'warm')\n",
      "(134, 'potter', '->', 37, 'them')\n",
      "(134, 'potter', '->', 3042, 'pleasantly')\n",
      "(2, 'and', '->', 1, 'the')\n",
      "(2, 'and', '->', 69, 'would')\n",
      "(2, 'and', '->', 41, 'been')\n",
      "(2, 'and', '->', 17, 'at')\n",
      "(2, 'and', '->', 3333, 'inner')\n",
      "(2, 'and', '->', 2550, 'gargoyle')\n",
      "(2, 'and', '->', 134, 'potter')\n",
      "(2, 'and', '->', 101, 'going')\n",
      "(2, 'and', '->', 48, 'who')\n",
      "(2, 'and', '->', 912, 'reason')\n",
      "(2, 'and', '->', 1557, 'list')\n",
      "(2, 'and', '->', 199, 'let')\n",
      "(1, 'the', '->', 2, 'and')\n",
      "(1, 'the', '->', 323, 'quickly')\n",
      "(1, 'the', '->', 65, 'do')\n",
      "(1, 'the', '->', 34, 'all')\n",
      "(1, 'the', '->', 202, 'three')\n",
      "(1, 'the', '->', 1539, 'fresh')\n",
      "(1, 'the', '->', 2788, 'sorcerers')\n",
      "(1, 'the', '->', 2076, 'horrified')\n",
      "(1, 'the', '->', 36, 'have')\n",
      "(1, 'the', '->', 109, 'didnt')\n",
      "(1, 'the', '->', 522, 'lets')\n",
      "(1, 'the', '->', 733, 'nodded')\n",
      "(2788, 'sorcerers', '->', 347, 'stone')\n",
      "(2788, 'sorcerers', '->', 682, 'killed')\n",
      "(2788, 'sorcerers', '->', 52, 'now')\n",
      "(2788, 'sorcerers', '->', 11, 'it')\n",
      "(2788, 'sorcerers', '->', 105, 'look')\n",
      "(2788, 'sorcerers', '->', 40, 'there')\n",
      "(2788, 'sorcerers', '->', 1, 'the')\n",
      "(2788, 'sorcerers', '->', 946, 'throat')\n",
      "(2788, 'sorcerers', '->', 7962, 'knotted')\n",
      "(2788, 'sorcerers', '->', 131, 'other')\n",
      "(2788, 'sorcerers', '->', 23, 'they')\n",
      "(2788, 'sorcerers', '->', 6212, 'talented')\n"
     ]
    }
   ],
   "source": [
    "positiveSampleSize = 8\n",
    "kNegativeSamples = 5\n",
    "skipWindow = 1\n",
    "batch, labels = generateBatch(positiveSampleSize=positiveSampleSize, skipWindow=skipWindow, \n",
    "                              kNegativeSamples = kNegativeSamples)\n",
    "\n",
    "for i in range(len(batch)):\n",
    "    print(batch[i][0], ranksToWords[batch[i][0]],\n",
    "        '->', batch[i][1], ranksToWords[batch[i][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopefully, the core of the implementation is correct (the code below). Now what is needed is to pass data to this function in batches and design the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## SOME CONSTANTS \n",
    "\n",
    "# Will be modified as a global variable by the generateBatch function  \n",
    "dataIndex = 0\n",
    "# Experimental Theano Code (psuedo code)\n",
    "batchSize = positiveSampleSize*(1 + kNegativeSamples)\n",
    "\n",
    "###############################\n",
    "### THE CORE IMPLEMENTATION ###\n",
    "###############################\n",
    "\n",
    "# the W matrix of the inputVectors as used in [1]\n",
    "targetEmbeddings = theano.shared(np.random.uniform(-1, 1, (vocabSize, embeddingSize)))\n",
    "# the W' matrix of the outputVectors as used in [1]\n",
    "contextEmbeddings = theano.shared(np.random.normal(scale = 1.0/np.sqrt(vocabSize), \n",
    "                                                   size = (embeddingSize, vocabSize)))\n",
    "\n",
    "# A |batchSize x 2| dimensional matrix, having (traget, context) pairs for\n",
    "# a batch (including) -ve samples. This is the input to the training function .\n",
    "targetContext = T.imatrix()\n",
    "\n",
    "# the |batchSize x 1| vector, trainig labels (also an input to the training\n",
    "# function), whether the context word matches the target word or not\n",
    "isContext = T.bvector()\n",
    "\n",
    "batchMatchScores = []\n",
    "\n",
    "for i in range(batchSize):\n",
    "    matchScore = T.dot(targetEmbeddings[targetContext[i][0],:], contextEmbeddings[:,targetContext[i][1]])\n",
    "    batchMatchScores.append(matchScore)\n",
    "\n",
    "objective = isContext*T.log(T.nnet.sigmoid(batchMatchScores)) + \\\n",
    "                (1 - isContext)*T.log(1-T.nnet.sigmoid(batchMatchScores))\n",
    "\n",
    "loss = -T.mean(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "from lasagne.updates import nesterov_momentum\n",
    "updates = nesterov_momentum(loss, [targetEmbeddings, contextEmbeddings], learning_rate = 0.1, momentum = 0.9)\n",
    "trainBatch = theano.function([targetContext, isContext], loss, updates = updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numberOfBatches = len(data)/(positiveSampleSize/(2*skipWindow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271451"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberOfBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 complete.\n",
      "Batch 1 complete.\n",
      "Batch 2 complete.\n",
      "Batch 3 complete.\n",
      "Batch 4 complete.\n",
      "Batch 5 complete.\n",
      "Batch 6 complete.\n",
      "Batch 7 complete.\n",
      "Batch 8 complete.\n",
      "Batch 9 complete.\n",
      "Batch 10 complete.\n",
      "Batch 11 complete.\n",
      "Batch 12 complete.\n",
      "Batch 13 complete.\n",
      "Batch 14 complete.\n",
      "Batch 15 complete.\n",
      "Batch 16 complete.\n",
      "Batch 17 complete.\n",
      "Batch 18 complete.\n",
      "Batch 19 complete.\n",
      "Batch 20 complete.\n",
      "Batch 21 complete.\n",
      "Batch 22 complete.\n",
      "Batch 23 complete.\n",
      "Batch 24 complete.\n",
      "Batch 25 complete.\n",
      "Batch 26 complete.\n",
      "Batch 27 complete.\n",
      "Batch 28 complete.\n",
      "Batch 29 complete.\n",
      "Batch 30 complete.\n",
      "Batch 31 complete.\n",
      "Batch 32 complete.\n",
      "Batch 33 complete.\n",
      "Batch 34 complete.\n",
      "Batch 35 complete.\n",
      "Batch 36 complete.\n",
      "Batch 37 complete.\n",
      "Batch 38 complete.\n",
      "Batch 39 complete.\n",
      "Batch 40 complete.\n",
      "Batch 41 complete.\n",
      "Batch 42 complete.\n",
      "Batch 43 complete.\n",
      "Batch 44 complete.\n",
      "Batch 45 complete.\n",
      "Batch 46 complete.\n",
      "Batch 47 complete.\n",
      "Batch 48 complete.\n",
      "Batch 49 complete.\n",
      "Batch 50 complete.\n",
      "Batch 51 complete.\n",
      "Batch 52 complete.\n",
      "Batch 53 complete.\n",
      "Batch 54 complete.\n",
      "Batch 55 complete.\n",
      "Batch 56 complete.\n",
      "Batch 57 complete.\n",
      "Batch 58 complete.\n",
      "Batch 59 complete.\n",
      "Batch 60 complete.\n",
      "Batch 61 complete.\n",
      "Batch 62 complete.\n",
      "Batch 63 complete.\n",
      "Batch 64 complete.\n",
      "Batch 65 complete.\n",
      "Batch 66 complete.\n",
      "Batch 67 complete.\n",
      "Batch 68 complete.\n",
      "Batch 69 complete.\n",
      "Batch 70 complete.\n",
      "Batch 71 complete.\n",
      "Batch 72 complete.\n",
      "Batch 73 complete.\n",
      "Batch 74 complete.\n",
      "Batch 75 complete.\n",
      "Batch 76 complete.\n",
      "Batch 77 complete.\n",
      "Batch 78 complete.\n",
      "Batch 79 complete.\n",
      "Batch 80 complete.\n",
      "Batch 81 complete.\n",
      "Batch 82 complete.\n",
      "Batch 83 complete.\n",
      "Batch 84 complete.\n",
      "Batch 85 complete.\n",
      "Batch 86 complete.\n",
      "Batch 87 complete.\n",
      "Batch 88 complete.\n",
      "Batch 89 complete.\n",
      "Batch 90 complete.\n",
      "Batch 91 complete.\n",
      "Batch 92 complete.\n",
      "Batch 93 complete.\n",
      "Batch 94 complete.\n",
      "Batch 95 complete.\n",
      "Batch 96 complete.\n",
      "Batch 97 complete.\n",
      "Batch 98 complete.\n",
      "Batch 99 complete.\n",
      "Batch 100 complete.\n",
      "Batch 101 complete.\n",
      "Batch 102 complete.\n",
      "Batch 103 complete.\n",
      "Batch 104 complete.\n",
      "Batch 105 complete.\n",
      "Batch 106 complete.\n",
      "Batch 107 complete.\n",
      "Batch 108 complete.\n",
      "Batch 109 complete.\n",
      "Batch 110 complete.\n",
      "Batch 111 complete.\n",
      "Batch 112 complete.\n",
      "Batch 113 complete.\n",
      "Batch 114 complete.\n",
      "Batch 115 complete.\n",
      "Batch 116 complete.\n",
      "Batch 117 complete.\n",
      "Batch 118 complete.\n",
      "Batch 119 complete.\n",
      "Batch 120 complete.\n",
      "Batch 121 complete.\n",
      "Batch 122 complete.\n",
      "Batch 123 complete.\n",
      "Batch 124 complete.\n",
      "Batch 125 complete.\n",
      "Batch 126 complete.\n",
      "Batch 127 complete.\n",
      "Batch 128 complete.\n",
      "Batch 129 complete.\n",
      "Batch 130 complete.\n",
      "Batch 131 complete.\n",
      "Batch 132 complete.\n",
      "Batch 133 complete.\n",
      "Batch 134 complete.\n",
      "Batch 135 complete.\n",
      "Batch 136 complete.\n",
      "Batch 137 complete.\n",
      "Batch 138 complete.\n",
      "Batch 139 complete.\n",
      "Batch 140 complete.\n",
      "Batch 141 complete.\n",
      "Batch 142 complete.\n",
      "Batch 143 complete.\n",
      "Batch 144 complete.\n",
      "Batch 145 complete.\n",
      "Batch 146 complete.\n",
      "Batch 147 complete.\n",
      "Batch 148 complete.\n",
      "Batch 149 complete.\n",
      "Batch 150 complete.\n",
      "Batch 151 complete.\n",
      "Batch 152 complete.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2176a8570560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrainBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Batch {0} complete.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataIndex = 0\n",
    "for i in range(numberOfBatches):\n",
    "    batch, labels = generateBatch(positiveSampleSize, skipWindow, kNegativeSamples)\n",
    "    batch = np.asarray(batch, dtype = np.uint16)\n",
    "    labels = np.asarray(labels, dtype = np.int8)\n",
    "    trainBatch(batch, labels)\n",
    "    print 'Batch {0} complete.'.format(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_metadata": {
   "affiliation": "Department of CSE, Indian Institute of Technology(BHU) - Varanasi, India",
   "author": "Hritik Jain"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
